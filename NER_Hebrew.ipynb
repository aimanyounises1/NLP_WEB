{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Hebrew.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3ttUKgZ0624uMuJoGmGIX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimanyounises1/NLP_WEB/blob/master/NER_Hebrew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnku7VQ37nZ_",
        "outputId": "9416b1cb-7f75-4c7b-8d73-646fdb97ee21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cuda\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWBEyuyy74AB",
        "outputId": "cf17d1df-b9d8-4c4a-d069-3ff0089ba978"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement cuda (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for cuda\u001b[0m\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsqprQFQ75wz",
        "outputId": "fc135ac8-ead0-4725-c840-a853cd4f5d4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "train_path = \"/content/dataset_hebrew.csv\"\n",
        "data = pd.read_csv(train_path, encoding=\"utf8\").fillna(method=\"ffill\")"
      ],
      "metadata": {
        "id": "MG7mOCCZ9of0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data[\"Tag\"] = data[\"Tag\"].apply(lambda row : \n",
        "             row.split(\"^\")[0] if \"^\" in row else row                                \n",
        ")\n",
        "\n",
        "data , _ = train_test_split(data , test_size = 0.99 ,random_state=44 , shuffle= False)\n",
        "training_data, testing_data = train_test_split(data, test_size = 0.2, random_state=44 ,shuffle=False)\n",
        "plt.plot(data[\"Tag\"])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#labels = set(data[\"Tag\"].values)\n",
        "#print(labels)\n",
        "#data.head(50)\n",
        "words = list()\n",
        "labels = list()\n",
        "agg_func = lambda s: [\n",
        "            (w ,  t)  for w , t  in zip (s[\"Word\"].values.tolist(), s[\"Tag\"].values.tolist())\n",
        "                         \n",
        "        ]\n",
        "\n",
        "data_train = training_data.groupby(\"Sentence #\").apply(agg_func)\n",
        "data_test = testing_data.groupby(\"Sentence #\").apply(agg_func)\n",
        "\n",
        "#print(data_train[\"Tag\"]))\n",
        "print(data_test.head())\n",
        "\n",
        "#for index , row in data.iterrows():\n",
        " # if row[\"Sentence #\"]:\n",
        "  #  print(row[\"Sentence #\"])\n",
        "#data_train = list([( row[\"Word\"] + \"\\n\" if row[\"Sentence #\"] else  row[\"Word\"] ,row[\"Tag\"].split(\"^\")[0] if \"^\" in row[\"Tag\"] else row[\"Tag\"]) for index , row in training_data.iterrows()])\n",
        "#data_test = list([( row[\"Word\"] + \"\\n\" if row[\"Sentence #\"] else row[\"Word\"] ,row[\"Tag\"].split(\"^\")[0] if \"^\" in row[\"Tag\"] else row[\"Tag\"]) for index , row in testing_data.iterrows()])\n",
        "\n",
        "#print(data_train[:10])\n",
        "#print(data_test[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "XPpUpNEMY2fQ",
        "outputId": "2eca55b2-65b6-4a44-8f94-1c43b36d6a03"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence #\n",
            "Sentence: 100    [(מה, O), (בין, O), (זה, O), (לבין, O), (גינוי...\n",
            "Sentence: 101    [(ו, O), (מאמר, O), (מוסגר, O), (:, O), (ה, O)...\n",
            "Sentence: 102    [(אפשר, O), (למצוא, O), (את, O), (הוא, O), (ב,...\n",
            "Sentence: 103    [(יש, O), (גם, O), (כללים, O), (:, O), (לעולם,...\n",
            "Sentence: 104    [(הטפה, O), (ב, O), (נוסח, O), (\", O), (ו, O),...\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data[\"Tag\"].values.tolist())\n",
        "#labels = list()\n",
        "data.head()\n",
        "labels_list = set()\n",
        "data[\"Tag\"].apply(lambda row : \n",
        "                labels_list.add(row)         \n",
        ")\n",
        "\n",
        "labels_list = list(labels_list)\n",
        "#labels_list.remove(\"ה\")\n",
        "print(labels_list)\n",
        "\n",
        "#print(labels)\n",
        "#data[\"Tag\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W08Nev0lra2A",
        "outputId": "b5e9d4dd-89fe-4e4a-9ac3-b7e4e434b7eb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I-FAC', 'I-LOC', 'I-EVE', 'E-WOA', 'S-ORG', 'B-GPE', 'E-ORG', 'B-FAC', 'I-WOA', 'S-GPE', 'B-WOA', 'S-ANG', 'B-DUC', 'E-FAC', 'S-WOA', 'B-ORG', 'S-FAC', 'E-DUC', 'O', 'E-EVE', 'S-PER', 'B-LOC', 'B-EVE', 'B-PER', 'I-GPE', 'E-LOC', 'I-PER', 'S-EVE', 'S-DUC', 'I-DUC', 'E-PER', 'S-LOC', 'E-GPE', 'I-ORG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "def read_AnEM(path):\n",
        "    with open(path,'r',encoding='utf-8' , errors='ignore') as f:\n",
        "      data = []\n",
        "      sentence = []\n",
        "      label = []\n",
        "      for line in f:\n",
        "        if line=='\\n':\n",
        "          if len(sentence) > 0:\n",
        "            data.append((sentence,label))\n",
        "            sentence = []\n",
        "            label = []\n",
        "          continue\n",
        "        splits = line.split()\n",
        "        #print(splits)\n",
        "        sentence.append(splits[0])\n",
        "        label.append(splits[1])\n",
        "      if len(sentence) > 0:\n",
        "        data.append((sentence,label))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "kdjSFTg_BKuG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnfsGQ9KsvRH",
        "outputId": "4b5e6f7c-e6ec-45ba-9ce1-a5a24a5108cf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I-FAC', 'I-LOC', 'I-EVE', 'E-WOA', 'S-ORG', 'B-GPE', 'E-ORG', 'B-FAC', 'I-WOA', 'S-GPE', 'B-WOA', 'S-ANG', 'B-DUC', 'E-FAC', 'S-WOA', 'B-ORG', 'S-FAC', 'E-DUC', 'O', 'E-EVE', 'S-PER', 'B-LOC', 'B-EVE', 'B-PER', 'I-GPE', 'E-LOC', 'I-PER', 'S-EVE', 'S-DUC', 'I-DUC', 'E-PER', 'S-LOC', 'E-GPE', 'I-ORG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list\n",
        "\n",
        "all_datasets = []"
      ],
      "metadata": {
        "id": "V_Lz3QVmoruW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_AJGT = Dataset(\"NER_HE\", data_train, data_test, labels_list)\n",
        "all_datasets.append(data_AJGT)\n"
      ],
      "metadata": {
        "id": "y3HrEHxCoxbU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "9OB0Y3mTo1j-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name =\"NER_HE\"\n",
        "model_name = 'onlplab/alephbert-base'\n",
        "task_name = 'tokenclassification'"
      ],
      "metadata": {
        "id": "yR0rfJejpJCj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in all_datasets:\n",
        "  if d.name==dataset_name:\n",
        "    selected_dataset = d\n",
        "    print(selected_dataset.label_list)\n",
        "    print('Dataset found')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBApr7PApwh7",
        "outputId": "951e5d92-2cb3-46f8-cd74-972c18024ce5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I-FAC', 'I-LOC', 'I-EVE', 'E-WOA', 'S-ORG', 'B-GPE', 'E-ORG', 'B-FAC', 'I-WOA', 'S-GPE', 'B-WOA', 'S-ANG', 'B-DUC', 'E-FAC', 'S-WOA', 'B-ORG', 'S-FAC', 'E-DUC', 'O', 'E-EVE', 'S-PER', 'B-LOC', 'B-EVE', 'B-PER', 'I-GPE', 'E-LOC', 'I-PER', 'S-EVE', 'S-DUC', 'I-DUC', 'E-PER', 'S-LOC', 'E-GPE', 'I-ORG']\n",
            "Dataset found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset:\n",
        "  def __init__(self, texts, tags, label_list, model_name, max_length):\n",
        "    self.texts = texts\n",
        "    self.tags = tags\n",
        "    self.label_map = {label: i for i, label in enumerate(label_list)}\n",
        "    self.pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index\n",
        "    # Use cross entropy ignore_index as padding label id so that only\n",
        "    # real label ids contribute to the loss later.\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.max_length = max_length\n",
        "\n",
        "     \n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    textlist = self.texts[item]\n",
        "    tags = self.tags[item]\n",
        "    #print(\"tags\",tags)\n",
        "    #print(\"textlist\" , textlist)\n",
        "    tokens = []\n",
        "    label_ids = []\n",
        "    #for word, label in zip(textlist, tags): \n",
        "     # print(f\"Word : {word} , Label: {label}\")\n",
        "    label = tags\n",
        "    word = textlist \n",
        "    #print(f\"Word : {word} , Lable = {label}\") \n",
        "    word_tokens = self.tokenizer.tokenize(word)\n",
        "    if len(word_tokens) > 0:\n",
        "        tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "        label_ids.extend([self.label_map[label]] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
        "  \n",
        "    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "    special_tokens_count = self.tokenizer.num_special_tokens_to_add()\n",
        "    if len(tokens) > self.max_length - special_tokens_count:\n",
        "      tokens = tokens[: (self.max_length - special_tokens_count)]\n",
        "      label_ids = label_ids[: (self.max_length - special_tokens_count)]\n",
        "      \n",
        "    \n",
        "    #Add the [SEP] token\n",
        "    tokens += [self.tokenizer.sep_token]\n",
        "    label_ids += [self.pad_token_label_id]\n",
        "    token_type_ids = [0] * len(tokens)\n",
        "\n",
        "    #Add the [CLS] TOKEN\n",
        "    tokens = [self.tokenizer.cls_token] + tokens\n",
        "    label_ids = [self.pad_token_label_id] + label_ids\n",
        "    token_type_ids = [0] + token_type_ids\n",
        "\n",
        "    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = self.max_length - len(input_ids)\n",
        "\n",
        "    input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "    token_type_ids += [0] * padding_length\n",
        "    label_ids += [self.pad_token_label_id] * padding_length\n",
        "\n",
        "    assert len(input_ids) == self.max_length\n",
        "    assert len(attention_mask) == self.max_length\n",
        "    assert len(token_type_ids) == self.max_length\n",
        "    assert len(label_ids) == self.max_length\n",
        "\n",
        "    # if item < 5:\n",
        "    #   print(\"*** Example ***\")\n",
        "    #   print(\"tokens:\", \" \".join([str(x) for x in tokens]))\n",
        "    #   print(\"input_ids:\", \" \".join([str(x) for x in input_ids]))\n",
        "    #   print(\"attention_mask:\", \" \".join([str(x) for x in attention_mask]))\n",
        "    #   print(\"token_type_ids:\", \" \".join([str(x) for x in token_type_ids]))\n",
        "    #   print(\"label_ids:\", \" \".join([str(x) for x in label_ids]))\n",
        "    \n",
        "    return {\n",
        "        'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n",
        "        'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n",
        "        'token_type_ids' : torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        'labels' : torch.tensor(label_ids, dtype=torch.long)       \n",
        "    }"
      ],
      "metadata": {
        "id": "y6I8a4HwpXhB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "for i in range(len(selected_dataset.train)):\n",
        "    for x in selected_dataset.train[i]:\n",
        "      print(x[0])\n",
        "    break\n",
        "#for x in selected_dataset.test:\n",
        " # print(x[0] , x[1])\n",
        "  #break\n",
        " \n",
        "#print(selected_dataset.train[:10])\n",
        "train_dataset = NERDataset(\n",
        "    texts = [x[0] for i in range(len(selected_dataset.train)) for x in selected_dataset.train[i]],\n",
        "    tags = [x[1] for i in range(len(selected_dataset.train)) for x in selected_dataset.train[i]],\n",
        "    label_list = selected_dataset.label_list,\n",
        "    model_name = model_name,\n",
        "    max_length = 256\n",
        "    )\n",
        "\n",
        "test_dataset = NERDataset(\n",
        "    texts = [x[0] for i in range(len(selected_dataset.test)) for x in selected_dataset.test[i]],\n",
        "    tags = [x[1] for i in range(len(selected_dataset.test)) for x in selected_dataset.test[i]],\n",
        "    label_list=selected_dataset.label_list,\n",
        "    model_name=model_name,\n",
        "    max_length=256\n",
        "    )\n",
        "print(\"@@@@@@\")\n",
        "print(test_dataset.texts[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bMzBBV-pdk9",
        "outputId": "04598dc8-5e77-43c9-c8af-f30e55c66aa5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-FAC': 0, 'I-LOC': 1, 'I-EVE': 2, 'E-WOA': 3, 'S-ORG': 4, 'B-GPE': 5, 'E-ORG': 6, 'B-FAC': 7, 'I-WOA': 8, 'S-GPE': 9, 'B-WOA': 10, 'S-ANG': 11, 'B-DUC': 12, 'E-FAC': 13, 'S-WOA': 14, 'B-ORG': 15, 'S-FAC': 16, 'E-DUC': 17, 'O': 18, 'E-EVE': 19, 'S-PER': 20, 'B-LOC': 21, 'B-EVE': 22, 'B-PER': 23, 'I-GPE': 24, 'E-LOC': 25, 'I-PER': 26, 'S-EVE': 27, 'S-DUC': 28, 'I-DUC': 29, 'E-PER': 30, 'S-LOC': 31, 'E-GPE': 32, 'I-ORG': 33}\n",
            "עשרות\n",
            "אנשים\n",
            "מגיעים\n",
            "מ\n",
            "תאילנד\n",
            "ל\n",
            "ישראל\n",
            "כש\n",
            "הם\n",
            "נרשמים\n",
            "כ\n",
            "מתנדבים\n",
            ",\n",
            "אך\n",
            "למעשה\n",
            "משמשים\n",
            "עובדים\n",
            "שכירים\n",
            "זולים\n",
            ".\n",
            "\"\n",
            "תהיה\n",
            "נקמה\n",
            "ו\n",
            "בגדול\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e23adba9479ccc8d306218ed6daff9b06d1eed72091408bf4f936aca934fe3a0.68d0ec0be97fb095ce7b008c055f41d9e086d78cef089bacc9e3d6c4ac9957c1\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/7a51f707ff487db456bee52a63ab1397f3f54e0527bf853019f655cbbcc05f4b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f896c1feb1559b70f2b7d976d53a1715b384352f27ac5545f1aca6fa288b238f.9349993a068843616b15efde8b6b7aa4dbf7b857170167728e4b02649b1eae8d\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e23adba9479ccc8d306218ed6daff9b06d1eed72091408bf4f936aca934fe3a0.68d0ec0be97fb095ce7b008c055f41d9e086d78cef089bacc9e3d6c4ac9957c1\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/7a51f707ff487db456bee52a63ab1397f3f54e0527bf853019f655cbbcc05f4b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f896c1feb1559b70f2b7d976d53a1715b384352f27ac5545f1aca6fa288b238f.9349993a068843616b15efde8b6b7aa4dbf7b857170167728e4b02649b1eae8d\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@@@@@@\n",
            "['מה', 'בין', 'זה', 'לבין', 'גינוי', 'ציני', 'ל', 'ה', 'אופן', 'ש']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "def model_init():\n",
        "    return AutoModelForTokenClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "metadata": {
        "id": "Sinl4-kDsuLZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_label_map = {i: label for i, label in enumerate(labels_list)}\n",
        "\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "    #print(preds.shape)\n",
        "    batch_size, seq_len = preds.shape\n",
        "\n",
        "    out_label_list = [[] for _ in range(batch_size)]\n",
        "    preds_list = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
        "                out_label_list[i].append(inv_label_map[label_ids[i][j]])\n",
        "                preds_list[i].append(inv_label_map[preds[i][j]])\n",
        "\n",
        "    return preds_list, out_label_list\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds_list, out_label_list = align_predictions(p.predictions,p.label_ids)\n",
        "    #print(classification_report(out_label_list, preds_list,digits=4))\n",
        "    print(accuracy_score(out_label_list, preds_list))\n",
        "    return {\n",
        "        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
        "        \"precision\": precision_score(out_label_list, preds_list),\n",
        "        \"recall\": recall_score(out_label_list, preds_list),\n",
        "        \"f1\": f1_score(out_label_list, preds_list),\n",
        "    }"
      ],
      "metadata": {
        "id": "rlIRB8tUsz5_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 16\n",
        "training_args.per_device_eval_batch_size = 16\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= 8\n",
        "\n",
        "\n",
        "print(len(selected_dataset.train))\n",
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)\n",
        "#Warmup_ratio\n",
        "warmup_ratio = 0.1\n",
        "training_args.warmup_steps = total_steps*warmup_ratio\n",
        "\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000 #don't want to save any model\n",
        "training_args.seed = 42\n",
        "training_args.disable_tqdm = False\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsG8FTabs57z",
        "outputId": "975ff3e0-25a6-4f0a-a8cf-701ea3de516b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "15\n",
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=test_dataset, \n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gogI5wutCuj",
        "outputId": "21026d66-37cb-406b-f1fc-136ee18ee795"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\",\n",
            "    \"22\": \"LABEL_22\",\n",
            "    \"23\": \"LABEL_23\",\n",
            "    \"24\": \"LABEL_24\",\n",
            "    \"25\": \"LABEL_25\",\n",
            "    \"26\": \"LABEL_26\",\n",
            "    \"27\": \"LABEL_27\",\n",
            "    \"28\": \"LABEL_28\",\n",
            "    \"29\": \"LABEL_29\",\n",
            "    \"30\": \"LABEL_30\",\n",
            "    \"31\": \"LABEL_31\",\n",
            "    \"32\": \"LABEL_32\",\n",
            "    \"33\": \"LABEL_33\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_22\": 22,\n",
            "    \"LABEL_23\": 23,\n",
            "    \"LABEL_24\": 24,\n",
            "    \"LABEL_25\": 25,\n",
            "    \"LABEL_26\": 26,\n",
            "    \"LABEL_27\": 27,\n",
            "    \"LABEL_28\": 28,\n",
            "    \"LABEL_29\": 29,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_30\": 30,\n",
            "    \"LABEL_31\": 31,\n",
            "    \"LABEL_32\": 32,\n",
            "    \"LABEL_33\": 33,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/onlplab/alephbert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6493f23b67d8babed3f9f76188aa4a4d8736f4af9db4e0d6bbe9fd2ea167d5df.21794fd468017bb57065faa6d83d946c4e1a641e74e77cf3e82d7acad841e1c6\n",
            "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y_psGdTQtPp2",
        "outputId": "3f352ada-361b-46f3-bca1-9ac54f093c47"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\",\n",
            "    \"22\": \"LABEL_22\",\n",
            "    \"23\": \"LABEL_23\",\n",
            "    \"24\": \"LABEL_24\",\n",
            "    \"25\": \"LABEL_25\",\n",
            "    \"26\": \"LABEL_26\",\n",
            "    \"27\": \"LABEL_27\",\n",
            "    \"28\": \"LABEL_28\",\n",
            "    \"29\": \"LABEL_29\",\n",
            "    \"30\": \"LABEL_30\",\n",
            "    \"31\": \"LABEL_31\",\n",
            "    \"32\": \"LABEL_32\",\n",
            "    \"33\": \"LABEL_33\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_22\": 22,\n",
            "    \"LABEL_23\": 23,\n",
            "    \"LABEL_24\": 24,\n",
            "    \"LABEL_25\": 25,\n",
            "    \"LABEL_26\": 26,\n",
            "    \"LABEL_27\": 27,\n",
            "    \"LABEL_28\": 28,\n",
            "    \"LABEL_29\": 29,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_30\": 30,\n",
            "    \"LABEL_31\": 31,\n",
            "    \"LABEL_32\": 32,\n",
            "    \"LABEL_33\": 33,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/onlplab/alephbert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6493f23b67d8babed3f9f76188aa4a4d8736f4af9db4e0d6bbe9fd2ea167d5df.21794fd468017bb57065faa6d83d946c4e1a641e74e77cf3e82d7acad841e1c6\n",
            "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 12430\n",
            "  Num Epochs = 8\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 3104\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1504' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1504/3104 2:10:42 < 2:19:14, 0.19 it/s, Epoch 3.87/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1902\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trainer.model , \"HeBERT_NER.pth\")"
      ],
      "metadata": {
        "id": "l7ZF6ukPF_kT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model = torch.load(\"/content/HeBERT_NER.pth\" , map_location=torch.device('cpu'))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "text = '''שלום אני  דור לומד באוניבירסטת אריאל'''\n",
        "keys = list(label_map.keys())\n",
        "values = list(label_map.values())\n",
        "nlp = pipeline('ner',model = model , tokenizer = tokenizer)\n",
        "output = nlp(text)\n",
        "print(type(label_map))\n",
        "print(label_map)\n",
        "for entity in output:\n",
        "  print(entity)\n",
        "  split = entity['entity'].split(\"_\")\n",
        "  index = int(split[-1])\n",
        "  print(index)\n",
        "  print(keys[index])\n"
      ],
      "metadata": {
        "id": "7sBkgX5_MlGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6583f435-3ff8-454d-83d3-f66885a2ef6c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e23adba9479ccc8d306218ed6daff9b06d1eed72091408bf4f936aca934fe3a0.68d0ec0be97fb095ce7b008c055f41d9e086d78cef089bacc9e3d6c4ac9957c1\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/7a51f707ff487db456bee52a63ab1397f3f54e0527bf853019f655cbbcc05f4b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "loading file https://huggingface.co/onlplab/alephbert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f896c1feb1559b70f2b7d976d53a1715b384352f27ac5545f1aca6fa288b238f.9349993a068843616b15efde8b6b7aa4dbf7b857170167728e4b02649b1eae8d\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/onlplab/alephbert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c311cde38d67060cfab2730d54b583d4d7b55c2bf556914da310d274b806e592.6df48d87da51ccd2d7121eb1fd6ebc489d701a2baed5666032a314e019327cb0\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "{'I-FAC': 0, 'I-LOC': 1, 'I-EVE': 2, 'E-WOA': 3, 'S-ORG': 4, 'B-GPE': 5, 'E-ORG': 6, 'B-FAC': 7, 'I-WOA': 8, 'S-GPE': 9, 'B-WOA': 10, 'S-ANG': 11, 'B-DUC': 12, 'E-FAC': 13, 'S-WOA': 14, 'B-ORG': 15, 'S-FAC': 16, 'E-DUC': 17, 'O': 18, 'E-EVE': 19, 'S-PER': 20, 'B-LOC': 21, 'B-EVE': 22, 'B-PER': 23, 'I-GPE': 24, 'E-LOC': 25, 'I-PER': 26, 'S-EVE': 27, 'S-DUC': 28, 'I-DUC': 29, 'E-PER': 30, 'S-LOC': 31, 'E-GPE': 32, 'I-ORG': 33}\n",
            "{'entity': 'LABEL_18', 'score': 0.9147705, 'index': 1, 'word': 'שלום', 'start': 0, 'end': 4}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_18', 'score': 0.8815814, 'index': 2, 'word': 'אני', 'start': 5, 'end': 8}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_18', 'score': 0.8883878, 'index': 3, 'word': 'דור', 'start': 10, 'end': 13}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_18', 'score': 0.85252005, 'index': 4, 'word': 'לומד', 'start': 14, 'end': 18}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_4', 'score': 0.45898485, 'index': 5, 'word': 'באוני', 'start': 19, 'end': 24}\n",
            "4\n",
            "S-ORG\n",
            "{'entity': 'LABEL_6', 'score': 0.22698417, 'index': 6, 'word': '##ביר', 'start': 24, 'end': 27}\n",
            "6\n",
            "E-ORG\n",
            "{'entity': 'LABEL_18', 'score': 0.4835315, 'index': 7, 'word': '##סט', 'start': 27, 'end': 29}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_18', 'score': 0.2740072, 'index': 8, 'word': '##ת', 'start': 29, 'end': 30}\n",
            "18\n",
            "O\n",
            "{'entity': 'LABEL_18', 'score': 0.2803139, 'index': 9, 'word': 'אריאל', 'start': 31, 'end': 36}\n",
            "18\n",
            "O\n"
          ]
        }
      ]
    }
  ]
}